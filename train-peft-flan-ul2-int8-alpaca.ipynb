{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Flan-* for Alpaca\n",
    "\n",
    "using Lora and *int8* quantization\n",
    "\n",
    "For UL2 training needs a 40G GPU; Smaller models will train a 24G card (still looking into 4bit quant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using_model='flan-t5-small' # takes 3 hours to train on a titan\n",
    "using_model='flan-t5-xxl'\n",
    "#using_model='flan-ul2' # I don't have a pretrained adapter weights for this yet; out of llambda labs credits :-("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow for a different sharded model to be used; otherwise should be same,same\n",
    "# (model name for model, model name for tokenizer)\n",
    "\n",
    "if using_model == 'flan-ul2': # currently requires 40G GPU\n",
    "    model_name = ('google/flan-ul2','google/flan-ul2')\n",
    "    run_name = 'flanul2-lora-int8-alpaca'\n",
    "    dataset = 'johnrobinsn/alpaca-cleaned'\n",
    "    peft_name = 'flanul2-lora-int8-alpaca'\n",
    "elif using_model == 'flan-t5-xxl': # should run on 24G GPU for debugging\n",
    "    model_name = ('philschmid/flan-t5-xxl-sharded-fp16','google/flan-t5-xxl')\n",
    "    run_name = 'flant5xxl-lora-int8-alpaca'\n",
    "    dataset = 'johnrobinsn/alpaca-cleaned'\n",
    "    peft_name = 'flant5xxl-lora-int8-alpaca'\n",
    "elif using_model == 'flan-t5-small': # quick to train    \n",
    "    model_name = ('google/flan-t5-small','google/flan-t5-small')\n",
    "    run_name = 'flant5small-lora-int8-alpaca'\n",
    "    dataset = 'johnrobinsn/alpaca-cleaned'\n",
    "    peft_name = 'flant5small-lora-int8-alpaca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('google/flan-t5-xxl',\n",
       " 'johnrobinsn/alpaca-cleaned',\n",
       " 'flant5xxl-lora-int8-alpaca',\n",
       " 'flant5xxl-lora-int8-alpaca')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name[1],dataset,peft_name,run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on lambdalabs bits and bytes couldn't find the cuda runtime so add ld path\n",
    "# import os\n",
    "\n",
    "# #if use_flan_ul2: # I'm using llambda labs for training ul2\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/usr/lib/x86_64-linux-gnu/'\n",
    "# os.getenv('LD_LIBRARY_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install training dependences\n",
    "!pip install -Uqq  git+https://github.com/huggingface/peft.git\n",
    "!pip install -Uqq \"transformers==4.27.1\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"bitsandbytes==0.37.1\" loralib --upgrade --quiet\n",
    "!pip install -Uqq wandb\n",
    "!pip install -Uqq protobuf==3.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If you just want to do inference you can jump all the way down to the [\"Evaluate\"](#evaluate) cell and start running from there to download my adapter weights from hf hub and try it out.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_to = \"wandb\" # \"none\"\n",
    "\n",
    "if report_to != \"none\":\n",
    "    import wandb\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=run_name,config={\n",
    "    \"model\": model_name[1],\n",
    "    \"dataset\":dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    # sorry about the formatting disaster gotta move fast\n",
    "    if data_point[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{data_point[\"input\"]}\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\"Loading tokenizer for model: \", model_name[1])\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x, tokenizer):\n",
    "    prompt = generate_prompt(x)\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    labels = tokenizer(text_target=x[\"output\"], max_length=CUTOFF_LEN, padding=\"max_length\", truncation=True)\n",
    "    # loss function will ignore -100 tokens\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    labels[\"input_ids\"] = [(l if l != tokenizer.pad_token_id else -100) for l in labels[\"input_ids\"]]\n",
    "             \n",
    "    return {\n",
    "        \"input_ids\": result[\"input_ids\"],\n",
    "        \"attention_mask\": result[\"attention_mask\"],\n",
    "        \"labels\": labels[\"input_ids\"]\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "data = load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SET_SIZE = 2000\n",
    "CUTOFF_LEN = 256  # 256 accounts for about 96% of the data\n",
    "\n",
    "\n",
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=VAL_SET_SIZE, shuffle=True, seed=42\n",
    ")\n",
    "train_data = train_val[\"train\"]\n",
    "val_data = train_val[\"test\"]\n",
    "\n",
    "train_data = train_data.shuffle().map(lambda x: tokenize(x, tokenizer))\n",
    "val_data = val_data.shuffle().map(lambda x: tokenize(x, tokenizer))\n",
    "\n",
    "# print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "# print(f\"Test dataset size: {len(dataset['test'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "print(\"Loading model for model: \", model_name[0])\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name[0], load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune Flan-UL2 with LoRA and int8\n",
    "\n",
    "We could shard the Flan-UL2 model to conserve memory while loading the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can prepare our model for the LoRA int-8 training using `peft`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# Define LoRA Config \n",
    "lora_config = LoraConfig(\n",
    " r= 8, #16, \n",
    " lora_alpha=16, #32,\n",
    " target_modules=[\"q\", \"v\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=TaskType.SEQ_2_SEQ_LM\n",
    " #task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# trainable params: 18874368 || all params: 11154206720 || trainable%: 0.16921300163961817"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "eval_steps = 200\n",
    "save_steps = 200\n",
    "logging_steps = 20\n",
    "output_dir = 'results'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=Seq2SeqTrainingArguments(\n",
    "        num_train_epochs=5,\n",
    "        learning_rate=3e-4,\n",
    "        logging_steps=logging_steps,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_steps=save_steps,\n",
    "        output_dir=output_dir,\n",
    "        report_to=report_to if report_to else \"none\",\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "    ),\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train our model and run the cells below. Note that for T5 and UL2, some layers are kept in `float32` for stability purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our LoRA model & tokenizer results\n",
    "trainer.model.save_pretrained(peft_name)\n",
    "tokenizer.save_pretrained(peft_name)\n",
    "# if you want to save the base model to call\n",
    "# trainer.model.base_model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq huggingface_hub\n",
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On lambdalabs had to run this in a terminal outside of the notebook\n",
    "# sudo apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = f'{huggingface_hub.whoami()[\"name\"]}/{peft_name}'\n",
    "trainer.model.push_to_hub(repo_id)\n",
    "tokenizer.push_to_hub(repo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Up Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "config = None\n",
    "model = None\n",
    "tokenizer=None\n",
    "trainer=None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate <a id=\"evaluate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/jr/anaconda3/envs/flan_t5_xxl/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/jr/anaconda3/envs/flan_t5_xxl/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer,T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Adapter Weights\n",
    "model_id = model_name[1] #'google/flan-ul2'\n",
    "#peft_model_id = peft_name # use locally saved adapter weights if you trained above\n",
    "peft_model_id = f'johnrobinsn/{peft_name}' # use my pretrained adapter weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('google/flan-t5-xxl', 'johnrobinsn/flant5xxl-lora-int8-alpaca')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id,peft_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a40d2c015141869c6415e45171f2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load peft config for pre-trained checkpoint etc. \n",
    "#peft_model_id = \"results\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id,  load_in_8bit=True,  device_map={\"\":0})\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(model_id,  load_in_8bit=True,  device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "model.eval()\n",
    "\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it out\n",
    "Let’s load the dataset again with a random sample to try the summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    # sorry about the formatting disaster gotta move fast\n",
    "    if data_point[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{data_point[\"input\"]}\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(instruction,input=None):\n",
    "    prompt = generate_prompt({'instruction':instruction,'input':input})\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "    outputs = model.generate(input_ids=input_ids, max_new_tokens=1000, do_sample=True, top_p=0.9)\n",
    "    print(f\"summary:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary:\n",
      "The protagonist was faced with an important career decision. He had been working in his current position for years, but the job was not satisfying him. He felt like he needed to make a change to get a more challenging and fulfilling job. After much consideration, he decided to explore his options and find another job.\n"
     ]
    }
   ],
   "source": [
    "generate('Write a short story in third person narration about a protagonist who has to make an important career decision.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary:\n",
      "The first man to walk on the moon was Neil Armstrong in 1969.\n"
     ]
    }
   ],
   "source": [
    "generate('Who was the first man to walk on the moon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary:\n",
      "The fraction 4/16 is equivalent to 1/4 because 1/16 x 2 equals 1/4.\n"
     ]
    }
   ],
   "source": [
    "generate('Explain why the following fraction is equivalent to 1/4','4/16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary:\n",
      "When she walks down the street, A cat's purr's like a waterfall The soft silky fur of her tail fluttering in the breeze Her whiskers swish in the breeze and she is my purrr-fect cat The soft meows of her fur meows loudly Bringing joy to those who are blessed with her presence And purr's like a heart of gold Her whimper meows can comfort the weary heart.\n"
     ]
    }
   ],
   "source": [
    "generate('Write a poem about about a cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4eddb440959a89d6755541b4ddad18aa547193e63575f384aa5d9491d8be2537"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
